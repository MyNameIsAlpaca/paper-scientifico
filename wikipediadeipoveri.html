
<!-- Pagina povera di wikipedia creata per diletto al di fuori dell'esercizio -->

<!DOCTYPE html>
<html lang="it">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Wikipedia</title>
</head>
<body>

    <!-- Parte iniziale della pagina -->
    <header>
        <h1>Wikipedia</h1>
        <h2>Support vector machine</h2>
        <hr>
        <span>From Wikipedia, the free encyclopedia</span>
    </header>
    <!-- /Parte iniziale della pagina -->

    <!-- corpo principale della pagina -->
    <main>
        <div>
            <p>
                In machine learning, <strong>support vector machines</strong> (SVMs, also support vector networks) are supervised learning models with associated learning algorithms that analyze data for classification and regression analysis. Developed at AT&T Bell Laboratories by Vladimir Vapnik with colleagues (Boser et al., 1992, Guyon et al., 1993, Cortes and Vapnik, 1995, Vapnik et al., 1997 SVMs are one of the most robust prediction methods, being based on statistical learning frameworks or VC theory proposed by Vapnik (1982, 1995) and Chervonenkis (1974). Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that assigns new examples to one category or the other, making it a non-probabilistic binary linear classifier (although methods such as Platt scaling exist to use SVM in a probabilistic classification setting). SVM maps training examples to points in space so as to maximise the width of the gap between the two categories. New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall.
                In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces.
                When data are unlabelled, supervised learning is not possible, and an unsupervised learning approach is required, which attempts to find natural clustering of the data to groups, and then map new data to these formed groups. The support vector clustering algorithm, created by Hava Siegelmann and Vladimir Vapnik, applies the statistics of support vectors, developed in the support vector machines algorithm, to categorize unlabeled data
            </p>
        </div>
        <div>
            <h4>
                Contents
            </h4>
            <ol>
                <li>Motivation</li>
                <li>Applications</li>
                <li>History</li>
            </ol>
        </div>
        <hr>
        <div>
            <h3>
                Motivation
            </h3>
            <p>
                Lorem ipsum, dolor sit amet consectetur adipisicing elit. Aliquam quidem quia magnam sed, quod voluptate. Asperiores magni laborum doloremque minus facilis. Odit, quibusdam. Cum, ducimus eligendi harum modi dolor laudantium!
            </p>
            <hr>
            <h3>
                Applications
            </h3>
            <span>SVMs can be used to solve various real-world problems:</span>
            <ul>
                <li>SVMs are helpful in text and hypertext categorization, as their application can significantly reduce the need for labeled training instances in both the standard inductive and transductive settings. Some methods for shallow semantic parsing are based on support vector machines.</li>
                <li>Classification of images can also be performed using SVMs. Experimental results show that SVMs achieve significantly higher search accuracy than traditional query refinement schemes after just three to four rounds of relevance feedback. This is also true for image segmentation systems, including those using a modified version SVM that uses the privileged approach as suggested by Vapnik.</li>
                <li>Classification of satellite data like SAR data using supervised SVM.</li>
                <li>Hand-written characters can be recognized using SVM.</li>
                <li>The SVM algorithm has been widely applied in the biological and other sciences. They have been used to classify proteins with up to 90% of the compounds classified correctly. Permutation tests based on SVM weights have been suggested as a mechanism for interpretation of SVM models. Support vector machine weights have also been used to interpret SVM models in the past. Posthoc interpretation of support vector machine models in order to identify features used by the model to make predictions is a relatively new area of research with special significance in the biological sciences.</li>
            </ul>
            <hr>
            <h3>
                History
            </h3>
            <span>
                The original SVM algorithm was invented by Vladimir N. Vapnik and Alexey Ya. Chervonenkis in 1964. In 1992, Bernhard Boser, Isabelle Guyon and Vladimir Vapnik suggested a way to create nonlinear classifiers by applying the kernel trick to maximum-margin hyperplanes. The "soft margin" incarnation, as is commonly used in software packages, was proposed by Corinna Cortes and Vapnik in 1993 and published in 1995.
            </span>
        </div>
    </main>
    <!-- /corpo principale della pagina -->

    <!-- parte finale della pagina -->
    <footer>
        <p align="center" >Made with &#10084; from MyNameIsAlpaca.inc</p>
    </footer>
    <!-- parte finale della pagina -->
</body>
</html>